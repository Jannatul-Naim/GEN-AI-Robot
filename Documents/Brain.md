# ðŸ¤– Robot Brain Module

## Overview
The **Robot Brain** module implements the cognitive layer of a robotic arm system. It integrates:

- **Vision perception**  
- **Large Language Model (LLM) reasoning**  
- **Short-term memory**  
- **Symbolic planning**

The brain converts **natural language commands** into **safe, executable robotic actions**.

---

## Architecture

User Command
â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Brain â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Memory â”‚ â† safety & state
â”‚ LLM â”‚ â† intent + reasoning
â”‚ Planner â”‚ â† symbolic actions
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â†“
Execution Plan
â†“
Robot Controller

yaml
Copy code

---

## Directory Structure

vision/
â”‚
â”œâ”€â”€ main.py # CLI loop and system entry point
â”œâ”€â”€ brain.py # Cognitive decision-making core
â”œâ”€â”€ llm.py # LLM prompt + decision extraction
â”œâ”€â”€ memory.py # Robot short-term memory & safety state
â”œâ”€â”€ plan.py # Symbolic task planner
â”œâ”€â”€ server.py # Vision + robot communication layer
â”œâ”€â”€ config.py # System configuration
â”œâ”€â”€ init.py
â””â”€â”€ README.md

yaml
Copy code

---

## Core Components

### `config.py`
Defines system parameters such as:

- Vision server URL  
- Robot controller endpoint  
- LLM inference parameters  
- Confidence thresholds for object selection  

---

### `brain.py`
The central controller that:

- Checks safety state  
- Queries the LLM for intent and steps  
- Validates decisions against memory and vision  
- Produces structured execution plans  

Supported intents: `task`, `chat`, `stop`

---

### `llm.py`
Handles language grounding:

- Builds prompts enforcing safety and constraints  
- Extracts structured decisions from LLM responses  
- Prevents hallucination and enforces maximum two-step actions  

---

### `memory.py`
Tracks robot internal state:

- Held object  
- Safety mode (`normal` or `stop`)  

Prevents illegal actions such as picking while already holding.

---

### `plan.py`
Implements symbolic planning primitives:

- Object selection (`nearest`, `farthest`)  
- Pick, place, and give actions  
- Relative positioning rules  

---

### `server.py`
Communication bridge:

- Fetches object detections from the vision system  
- Sends validated plans to the robot controller  
- Handles network failures gracefully  

---

### `main.py`
Interactive command loop:

- Accepts natural language commands  
- Fetches vision data  
- Executes cognitive reasoning  
- Sends plans to the robot  

---

## Example Interaction

USER> pick the nearest bottle and place it on the right

BRAIN OUTPUT:
{
"intent": "task",
"plan": [
{"action": "pick", "object": "bottle"},
{"action": "place", "x": 10, "z": 25}
],
"reply": "Done"
}

yaml
Copy code

---

## Requirements

```bash
pip install requests